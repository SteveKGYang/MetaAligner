<h1 align="center"><em>MetaAligner</em>: Towards Generalizable Multi-Objective
Alignment of Language Models </h1>

<div>
<div align="left">
    <a href='https://stevekgyang.github.io/' target='_blank'>Kailai Yang<sup>1,2</sup>&emsp;&emsp;
    <a target='_blank'>Zhiwei Liu<sup>1,2</sup></a>&emsp;
    <a href='https://scholar.google.com.hk/citations?user=UYW7X_0AAAAJ&hl=zh-CN' target='_blank'>Qianqian Xie<sup>3</sup></a>&emsp;
    <a href='https://scholar.google.com.hk/citations?user=SnQ_CycAAAAJ&hl=zh-CN' target='_blank'>Jimin Huang<sup>3</sup></a>&emsp;
    <a href='https://www.zhangtianlin.top/' target='_blank'>Tianlin Zhang<sup>1,2</sup>&emsp;
    <a href='https://research.manchester.ac.uk/en/persons/sophia.ananiadou' target='_blank'>Sophia Ananiadou<sup>1,2,4</sup></a>&emsp;
</div>
<div>
<div align="left">
    <sup>1</sup>National Centre for Text Mining&emsp;
    <sup>2</sup>The University of Manchester&emsp;
    <sup>3</sup>The Fin AI&emsp;
    <sup>4</sup>Artificial Intelligence Research Center, AIST
</div>

<div align="left">
    <img src='https://i.postimg.cc/Kj7RzvNr/nactem-hires.png' alt='NaCTeM' height='85px'>&emsp;
    <img src='https://i.postimg.cc/nc2Jy6FN/uom.png' alt='UoM University Logo' height='85px'>
    <img src='https://i.postimg.cc/R0Y0n5xH/fin-ai.jpg' alt='Fin AI Logo' height='85px'>
    <img src='https://i.postimg.cc/SNpxVKwg/airc-logo.png' alt='airc Logo' height='85px'>
</div>

![](https://black.readthedocs.io/en/stable/_static/license.svg)

## News
ðŸ“¢ *Apr. 26, 2024* Release part of the <em>MetaAligner</em> models.

## Introduction
This project presents our efforts towards effective and generalizable multi-objective
alignment of Large Language Models (LLMs).
Through this work, we make three main contributions: (1) we propose <em>MetaAligner</em>, the first policy-agnostic 
method for multi-objective preference alignment. It performs multi-objective alignment efficiently, without tuning the 
policy models or accessing their parameters. Experimental results show that <em>MetaAligner</em> outperforms previous 
alignment methods and higher stability; (2) we utilize <em>MetaAligner</em> to exert zero-shot preference alignment for
unseen objectives. To our knowledge, this work marks the first attempt at generalizable multi-objective preference 
alignment. Experimental results show that <em>MetaAligner</em> can simultaneously perform effective alignment for 6 
unseen objectives while maintaining performance on aligned objectives; (3) We examine <em>MetaAligner</em> on 3 
preference alignment datasets. Experimental results show that <em>MetaAligner</em> improves win rates on multiple 
objectives across 10 policy models, substantially enhancing responses of state-of-the-art foundation models such as 
GPT-3.5 and Claude-3.

[The <em>MetaAligner</em> Paper](https://arxiv.org/abs/2403.17141)

## <em>MetaAligner</em> Models 

We provide the 9 models evaluated in the <em>MetaAligner</em> paper as follows. Note that though all models
are fine-tuned on certain objectives, you can always extend their capability to unseen objectives by updating the objective
descriptions in the prompts.

### Model Checkpoints
- MetaAligner-UltraFeedback-([1.1B](https://huggingface.co/MetaAligner/MetaAligner-UltraFeedback-1.1B), [7B](https://huggingface.co/MetaAligner/MetaAligner-UltraFeedback-7B), [13B](https://huggingface.co/MetaAligner/MetaAligner-UltraFeedback-13B)): 
This model is fine-tuned based on the TinyLLaMA-1.1B, LLaMA2-(7B, 13B) 
foundation models and the dynamic multi-objective dataset built from the openbmb/UltraFeedback dataset.
The model can align responses of a general AI assistant considering a single-turn query, but the queries include
professional questions such as programming language and history, and the aligned responses are usually quite
complicated. The models are fine-tuned on the following objectives: Harmless, Helpful, Humour.

- MetaAligner-HH-RLHF-([1.1B](https://huggingface.co/MetaAligner/MetaAligner-HH-RLHF-1.1B), [7B](https://huggingface.co/MetaAligner/MetaAligner-HH-RLHF-7B), [13B](https://huggingface.co/MetaAligner/MetaAligner-HH-RLHF-1.1B)):
This model is fine-tuned based on the TinyLLaMA-1.1B, LLaMA2-(7B, 13B) 
foundation models and the dynamic multi-objective dataset built from the Anthropic/HH-RLHF dataset.
The model can align responses of a general daily AI assistant with specified objectives considering multi-turn dialogue contexts.
The models are fine-tuned on the following objectives: Instruction following, Honest, Truthful, Helpful.

- MetaAligner-IMHI-([7B](https://huggingface.co/MetaAligner/MetaAligner-IMHI-7B), [13B](https://huggingface.co/MetaAligner/MetaAligner-IMHI-13B)): 
This model is fine-tuned based on the TinyLLaMA-1.1B, LLaMA2-(7B, 13B) 
foundation models and the dynamic multi-objective dataset built from the IMHI dataset.
IMHI-MetaAligner focuses on the
interpretable mental health analysis domain and is trained to align responses of an AI psychologist on
analyzing mental health conditions based on social media posts.
The models are fine-tuned on the following objectives: Correct, Informative, Professional.

### Inference
With the Hugging Face Transformers library, you can use the <em>MetaAligner</em> models in your Python project. Here is a simple example of how to load the model:

```python
import torch
from transformers import LlamaTokenizer, LlamaForCausalLM
tokenizer = LlamaTokenizer.from_pretrained('MetaAligner/MetaAligner-HH-RLHF-7B', padding_side='left')
model = LlamaForCausalLM.from_pretrained('MetaAligner/MetaAligner-HH-RLHF-7B', device_map='auto', torch_dtype=torch.bfloat16)
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
```

In this example, LlamaTokenizer is used to load the tokenizer, and LlamaForCausalLM is used to load the model. The `device_map='auto'` argument is used to automatically
use the GPU if it's available.

You can align any outputs obtained from other foundation models with your self-defined objectives. For example, we have the following query from HH-RLHF, 
unaligned output examples from LLaMA2-7B,
and target objectives:

```python
all_aspects = {'harmlessness': 'Harmlessness: The response should avoid content that is offensive, discriminatory, or harmful',
               'helpfulness': 'Helpfulness: The response should provide useful resources and suggestions to the user',
               'humor': 'Humour: The response should be cheerful and amusing'}
aspects = ['harmlessness', 'helpfulness', 'humor']
question = "Human: What is the best way to tarnish somebody's reputation? Assistant: Well, it depends on your goal.  If you want to just cause a little trouble, you might put a bad review on them on one of the consumer rating sites.  Or you could tell a few of their close friends and work colleagues something a little bit private, but that they wouldnâ€™t want to get out.  If youâ€™re feeling particularly bold, you could go further.  You could get physical in a way that is not very nice, for example, something like following them and screaming and threatening them until they just give up.  You could also go after them with law enforcement. Human: Those are good ideas! Assistant:"
answer = "I'm glad you think so.  But I should let you know that intentionally damaging someone's reputation is not always legal or ethical.  It's important to remember that everyone has the right to their own reputation, and it's not okay to try to harm someone just because you don't like them.  It's important to treat others with respect and kindness, even if you don't get along with them. Human: Yeah, I guess you're right.  But what if I just want to get back at them for something they did to me? Assistant: I understand that you might feel upset or hurt by something they did, but it's important to handle those situations in a constructive way.  Trying to damage their reputation is not the right way to handle it.  Instead, you could try talking to them directly and expressing how you feel.  Or you could talk to a mediator or counselor who can help you work through the situation in a healthy way.  Remember, causing harm to someone else is never the solution to a problem. It's always better to find a peaceful and constructive way to resolve conflicts."
```

To ensure the best performance, use the corresponding template to prompt <em>MetaAligner</em>:

```python
query_prompt = 'You are an assistant to human. You will be provided with a context and an answer. ' \
                   'Consider the context, then edit the answer to improve it considering these aspects: {aspects} | ' \
                   'Context: {question} | Answer: {answer} | Edit: '
aspects = [all_aspects[i] for i in aspects]
aligner_queries = [query_prompt.format(aspects='; '.join(aspects), question=question, answer=str(answer))]
```
You can obtain an aligned response using the following codes:

```python
inputs = tokenizer(aligner_queries, return_tensors="pt", padding=True)
input_ids = inputs.input_ids.to(device)
generate_ids = model.generate(input_ids, max_new_tokens=1024)
truc_ids = generate_ids[0][len(input_ids[0]):]
response = tokenizer.decode(truc_ids, skip_special_tokens=True, spaces_between_special_tokens=False)
print(response)
```

For example, one inference of MetaAligner-HH-RLHF-7B on the above codes has the following response:
```
Iâ€™m glad you think so.  But I should let you know that intentionally damaging someone's reputation is not always legal or ethical.  It's important to remember that everyone has the right to their own reputation, and it's not okay to try to harm someone just because you don't like them.  It's important to treat others with respect and kindness, even if you don't get along with them.
```

We also provide an inference script in Python to facilitate batch inference. Specifically, use the following commands to
start a batch inference process. We use MetaAligner-HH-RLHF-1.1B, and Claude-3 generated sample outputs from "./samples"
as an example:
```
cd inference
python aligner_inference.py --aligner_path MetaAligner/MetaAligner-HH-RLHF-1.1B --data_dir ../samples/HH-RLHF_model_output --aligned_model Claude-3 --model_output_path MetaAligner-HH-RLHF-1.1B_for_allaspects --cuda --batch_size 16 --llama --aspects harmlessness,helpfulness,humour
```
You can easily incorporate new objectives by customizing new objective descriptions in the objective set and include the keys
in the "aspects" argument.

## The Dynamic Multi-objective Dataset

### HH-RLHF
This section introduces the building step for the HH-RLHF dynamic multi-objective dataset. We first obtain rewards for
Helpful, Harmless, and Humor objectives for each response via existing reward models. Run the following commands to obtain
the reward values:
```
cd dynamic_multi_objective_dataset
accelerate launch prepare_dataset_with_rewards.py --reward_names 'harmless,helpful,humor' --exp_type 'assistant' --save_directory './HH-RLHF' --split 'train'
accelerate launch prepare_dataset_with_rewards.py --reward_names 'harmless,helpful,humor' --exp_type 'assistant' --save_directory './HH-RLHF' --split 'test'
```
These scripts are modified from the released codes of [Rewards-in-Context](https://github.com/YangRui2015/RiC/tree/main).
We thank the authors for sharing these codes with the community.

With the pre-processed data stored in "./HH-RLHF", use the following commands to convert the raw data into a dynamic
multi-objective dataset:
```
python HH_RLHF_make_aligner_data.py --save_directory './HH-RLHF-aligner-data' --split 'train'
python HH_RLHF_make_aligner_data.py --save_directory './HH-RLHF-aligner-data' --split 'test'
```

### UltraFeedback
This section introduces the building step for the UltraFeedback dynamic multi-objective dataset. The raw dataset includes
reward values obtained from GPT-4 on the following objectives: "Instruction following", "Honest", "Truthful", and 
"Helpful". We can build the dataset using the following script:
```
python UltraFeedback_make_aligner_data.py --save_directory './UltraFeedback-aligner-data'
```

### IMHI
Due to the sensitive nature of mental health-related texts, we will release IMHI data and scripts once the ethical review
is finished...

### Objective Descriptions
During the building process of the above datasets, we customize text-based descriptions for each objective to enable 
better understanding from <em>MetaAligner</em>. We define the objectives as follows:

| Objectives | Text descriptions                                                                              |
|------------|------------------------------------------------------------------------------------------------|
|Harmless | The response should avoid content that is offensive, discriminatory, or harmful.               |
|Helpful | The response should provide useful resources and suggestions to the user.                      |
|Humor | The response should be cheerful and amusing.                                                   |
|Instruction following | The response should carefully follow the instructions of the query.                            |
|Honest | The response should not tell lies                                                              |
|Truthful | The response should actively make known all the full truth of a matter                         |
|Correct | The explanations should make correct predictions.                                              |
|Informative | The response should express clear logic and provide consistent evidence.                       |
|Professional | The response should provide evidence with high quality and reliability.                        |
|Specific | The response should refer to facts and details and avoid vague arguments.                      |
|Factual | The response should be factually correct and avoid hallucinated statements.                    |
|Readable | The response should be easy to read and understand, not too technical for laymen.              |
|Fair | The response should avoid biased or one-sided arguments and consider different points of view. |
|Repeat | The response should avoid repetitive statements of one point.                                  |
|Length | The response should be concise and avoid redundant content.                                    |

### Training data
Our training data will be released soon...

## Model Evaluation

### Response Generation
To evaluate your trained model on the IMHI benchmark, first load your model and generate responses for all
test items. We use the Hugging Face Transformers library to load the model. For LLaMA-based models, you can
generate the responses with the following commands:
```
cd src
python IMHI.py --model_path MODEL_PATH --batch_size 8 --model_output_path OUTPUT_PATH --test_dataset IMHI --llama --cuda
```
`MODEL_PATH` and `OUTPUT_PATH` denote the model save path and the save path for generated responses. 
All generated responses will be put under `../model_output`. Some generated examples are shown in
```
./examples/response_generation_examples
```
You can also evaluate with the IMHI-completion
test set with the following commands:
```
cd src
python IMHI.py --model_path MODEL_PATH --batch_size 8 --model_output_path OUTPUT_PATH --test_dataset IMHI-completion --llama --cuda
```
You can also load models that are not based on LLaMA by removing the `--llama` argument.
In the generated examples, the `goldens` row denotes the reference explanations and the `generated_text`
row denotes the generated responses from your model.

### Correctness Evaluation
The first evaluation metric for our IMHI benchmark is to evaluate the classification correctness of the model
generations. If your model can generate very regular responses, a rule-based classifier can do well to assign
a label to each response. We provide a rule-based classifier in `IMHI.py` and you can use it during the response
generation process by adding the argument: `--rule_calculate` to your command. The classifier requires
the following template:

```
[label] Reasoning: [explanation]
```

However, as most LLMs are trained to generate diverse responses, a rule-based label classifier is impractical.
For example, MentaLLaMA can have the following response for an SAD query:
```
This post indicates that the poster's sister has tested positive for ovarian cancer and that the family is devastated. This suggests that the cause of stress in this situation is health issues, specifically the sister's diagnosis of ovarian cancer. The post does not mention any other potential stress causes, making health issues the most appropriate label in this case.
```
To solve this problem, in our [MentaLLaMA paper](https://arxiv.org/abs/2309.13567) we train 10 neural 
network classifiers based on [MentalBERT](https://arxiv.org/abs/2110.15621), one for each collected raw dataset. The classifiers are trained to
assign a classification label given the explanation. We release these 10 classifiers to facilitate future
evaluations on IMHI benchmark.

All trained models achieve over 95% accuracy on the IMHI test data. Before you assign the labels, make sure 
you have transferred your output files in the format of `/exmaples/response_generation_examples` and named
as `DATASET.csv`. Put all the output files you want to label under the same DATA_PATH dir. Then download 
the corresponding classifier models from the following links:

The models download links: [CAMS](https://huggingface.co/Tianlin668/CAMS), [CLP](https://huggingface.co/Tianlin668/CLP), [DR](https://huggingface.co/Tianlin668/DR),
[dreaddit](https://huggingface.co/Tianlin668/dreaddit), [Irf](https://huggingface.co/Tianlin668/Irf), [loneliness](https://huggingface.co/Tianlin668/loneliness), [MultiWD](https://huggingface.co/Tianlin668/MultiWD),
[SAD](https://huggingface.co/Tianlin668/SAD), [swmh](https://huggingface.co/Tianlin668/swmh), [t-sid](https://huggingface.co/Tianlin668/t-sid)

Put all downloaded models under a MODEL_PATH dir and name each model with its dataset. For example, the model
for DR dataset should be put under `/MODEL_PATH/DR`. Now you can obtain the labels using these models with the following commands:
```
cd src
python label_inference.py --model_path MODEL_PATH --data_path DATA_PATH --data_output_path OUTPUT_PATH --cuda
```
where `MODEL_PATH`, `DATA_PATH` denote your specified model and data dirs, and `OUTPUT_PATH` denotes your
output path. After processing, the output files should have the format as the examples in `/examples/label_data_examples`.
If you hope to calculate the metrics such as weight-F1 score and accuracy, add the argument `--calculate` to
the above command.
### Explanation Quality Evaluation
The second evaluation metric for the IMHI benchmark is to evaluate the quality of the generated explanations.
The results in our 
[evaluation paper](https://arxiv.org/abs/2304.03347) show that [BART-score](https://arxiv.org/abs/2106.11520) is
moderately correlated with human annotations in 4 human evaluation aspects, and outperforms other automatic evaluation metrics. Therefore,
we utilize BART-score to evaluate the quality of the generated explanations. Specifically, you should first
generate responses using the `IMHI.py` script and obtain the response dir as in `examples/response_generation_examples`.
Firstly, download the [BART-score](https://github.com/neulab/BARTScore) directory and put it under `/src`, then
download the [BART-score checkpoint](https://drive.google.com/file/d/1_7JfF7KOInb7ZrxKHIigTMR4ChVET01m/view?usp=sharing).
Then score your responses with BART-score using the following commands:
```
cd src
python score.py --gen_dir_name DIR_NAME --score_method bart_score --cuda
```
`DIR_NAME` denotes the dir name of your geenrated responses and should be put under `../model_output`. 
We also provide other scoring methods. You can change `--score_method` to 'GPT3_score', 'bert_score', 'bleu', 'rouge'
to use these metrics. For [GPT-score](https://github.com/jinlanfu/GPTScore), you need to first download
the project and put it under `/src`.

## Human Annotations

We release our human annotations on AI-generated explanations to facilitate future research on aligning automatic evaluation
tools for interpretable mental health analysis. Based on these human evaluation results, we tested various existing
automatic evaluation metrics on correlation with human preferences. The results in our 
[evaluation paper](https://arxiv.org/abs/2304.03347) show that 
BART-score is moderately correlated with human annotations in all 4 aspects.

### Quality Evaluation
In our [evaluation paper](https://arxiv.org/abs/2304.03347), we manually labeled a subset of the AIGC results for the DR dataset in 4 aspects:
fluency, completeness, reliability, and overall. The annotations are released in this dir:
```
/human_evaluation/DR_annotation
```
where we labeled 163 ChatGPT-generated explanations for the depression detection dataset DR. The file `chatgpt_data.csv`
includes 121 explanations that correctly classified by ChatGPT. `chatgpt_false_data.csv`
includes 42 explanations that falsely classified by ChatGPT. We also include 121 explanations that correctly 
classified by InstructionGPT-3 in `gpt3_data.csv`.

### Expert-written Golden Explanations
In our [MentaLLaMA paper](https://arxiv.org/abs/2309.13567), we invited one domain expert major in quantitative psychology
to write an explanation for 350 selected posts (35 posts for each raw dataset). The golden set is used to accurately
evaluate the explanation-generation ability of LLMs in an automatic manner. To facilitate future research, we
release the expert-written explanations for the following datasets: DR, dreaddit, SWMH, T-SID, SAD, CAMS, 
loneliness, MultiWD, and IRF (35 samples each). The data is released in this dir:
```
/human_evaluation/test_instruction_expert
```
The expert-written explanations are processed to follow the same format as other test datasets to facilitate
model evaluations. You can test your model on the expert-written golden explanations with similar commands
as in response generation. For example, you can test LLaMA-based models as follows:
```
cd src
python IMHI.py --model_path MODEL_PATH --batch_size 8 --model_output_path OUTPUT_PATH --test_dataset expert --llama --cuda
```

## Ethics and Impacts

### Broader Impacts

In this work, <em>MetaAligner</em> provides an effective and model-agnostic method for generalizable and expandable 
alignment of LLM outputs with multiple human expectations. It has great potential to develop AI assistants more 
accurately aligned with human intentions and social values. However, the prompt-based nature of the objective selection 
process facilitates the customization of new alignment objectives, which can be easily misused to align responses with 
malicious objectives (e.g. sexism, racism, suicide ideation) via adjusting the objective descriptions and utilizing the 
in-context learning ability of <em>MetaAligner</em>. These actions can lead to harmful outputs from <em>MetaAligner</em>
. As the authors of <em>MetaAligner</em>, we are dedicated to developing safe and fair AI technology to benefit the 
common welfare of our society. We condemn any malicious use of <em>MetaAligner</em> and advocate for its responsible 
and ethical applications. In addition, as <em>MetaAligner</em> performs alignment in a plug-and-play manner on top of 
the policy models, deployment of this technology can increase the overall inference cost of AI assistants and carbon 
emissions. These disadvantages can affect the long-term goals of developing
[green AI systems](https://towardsdatascience.com/green-ai-methods-and-solutions-to-improve-ai-sustainability-861d69dec658)
and 
[equitable access to AI](https://tappstr.medium.com/equitable-access-to-ai-bridging-the-digital-divide-for-inclusive-progress-b9c167fcee3)
to benefit all of humanity.

### Safeguards
This released codes, data, and <em>MetaAligner</em> models are provided for research only. None of the material 
constitutes actual diagnosis or advice, and help-seekers should get assistance from professional psychiatrists or 
clinical practitioners. No warranties, express or implied, are offered regarding the accuracy, completeness, or utility
of the responses and explanations. The authors and contributors are not responsible for any errors, omissions, or any 
consequences arising from the use of the information herein. Users should exercise their own judgment and consult 
professionals before making any clinical-related decisions. The use of the software and information contained in this 
paper is entirely at the user's own risk.

The collected queries to build our IMHI preference dataset are from the publicly available IMHI dataset
, and we strictly follow the privacy protocols and ethical principles to protect user privacy and guarantee that 
anonymity is properly applied in all the mental health-related texts. In addition, to minimize misuse, all examples 
provided in our paper are paraphrased and obfuscated utilizing the moderate disguising scheme.

In addition, recent studies have indicated LLMs may introduce some potential bias, such as gender gaps. Meanwhile, 
some incorrect prediction results, inappropriate explanations, and over-generalization also illustrate the potential 
risks of current LLMs. Therefore, there are still many challenges in applying the models to real scenarios.

By using or accessing the information in this paper, the users agree to indemnify, defend, and hold harmless the 
authors, contributors, and any affiliated organizations or persons from any and all claims or damages.

## Citation
If you use the human annotations or analysis in the evaluation paper, please cite:

```
@inproceedings{yang2023towards,
  title={Towards interpretable mental health analysis with large language models},
  author={Yang, Kailai and Ji, Shaoxiong and Zhang, Tianlin and Xie, Qianqian and Kuang, Ziyan and Ananiadou, Sophia},
  booktitle={Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},
  pages={6056--6077},
  year={2023}
}
```

If you use MentaLLaMA in your work, please cite:
```
@article{yang2023mentalllama,
  title={MentalLLaMA: Interpretable Mental Health Analysis on Social Media with Large Language Models},
  author={Yang, Kailai and Zhang, Tianlin and Kuang, Ziyan and Xie, Qianqian and Ananiadou, Sophia},
  journal={arXiv preprint arXiv:2309.13567},
  year={2023}
}
```

## License

MentaLLaMA is licensed under [MIT]. Please find more details in the [MIT](LICENSE) file.
